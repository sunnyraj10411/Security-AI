{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.stats import norm, binom_test\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "from resnet import resnet as resnet_cifar\n",
    "\n",
    "#Adapted from https://github.com/locuslab/smoothing\n",
    "\n",
    "class Smooth(object):\n",
    "    \"\"\"A smoothed classifier g \"\"\"\n",
    "\n",
    "    # to abstain, Smooth returns this int\n",
    "    ABSTAIN = -1\n",
    "\n",
    "    def __init__(self, base_classifier: torch.nn.Module, num_classes: int, sigma: float):\n",
    "        \"\"\"\n",
    "        :param base_classifier: maps from [batch x channel x height x width] to [batch x num_classes]\n",
    "        :param num_classes:\n",
    "        :param sigma: the noise level hyperparameter\n",
    "        \"\"\"\n",
    "        self.base_classifier = base_classifier\n",
    "        self.num_classes = num_classes\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def certify(self, x: torch.tensor, n0: int, n: int, alpha: float, batch_size: int) -> (int, float):\n",
    "        \"\"\" Monte Carlo algorithm for certifying that g's prediction around x is constant within some L2 radius.\n",
    "        With probability at least 1 - alpha, the class returned by this method will equal g(x), and g's prediction will\n",
    "        robust within a L2 ball of radius R around x.\n",
    "\n",
    "        :param x: the input [channel x height x width]\n",
    "        :param n0: the number of Monte Carlo samples to use for selection\n",
    "        :param n: the number of Monte Carlo samples to use for estimation\n",
    "        :param alpha: the failure probability\n",
    "        :param batch_size: batch size to use when evaluating the base classifier\n",
    "        :return: (predicted class, certified radius)\n",
    "                 in the case of abstention, the class will be ABSTAIN and the radius 0.\n",
    "        \"\"\"\n",
    "        self.base_classifier.eval()\n",
    "        # draw samples of f(x+ epsilon)\n",
    "        counts_selection = self._sample_noise(x, n0, batch_size)\n",
    "        # use these samples to take a guess at the top class\n",
    "        cAHat = counts_selection.argmax().item()\n",
    "        # draw more samples of f(x + epsilon)\n",
    "        counts_estimation = self._sample_noise(x, n, batch_size)\n",
    "        # use these samples to estimate a lower bound on pA\n",
    "        nA = counts_estimation[cAHat].item()\n",
    "        pABar = self._lower_confidence_bound(nA, n, alpha)\n",
    "        if pABar < 0.5:\n",
    "            return Smooth.ABSTAIN, 0.0\n",
    "        else:\n",
    "            radius = self.sigma * norm.ppf(pABar)\n",
    "            return cAHat, radius\n",
    "\n",
    "    def predict(self, x: torch.tensor, n: int, alpha: float, batch_size: int) -> int:\n",
    "        \"\"\" Monte Carlo algorithm for evaluating the prediction of g at x.  With probability at least 1 - alpha, the\n",
    "        class returned by this method will equal g(x).\n",
    "\n",
    "        This function uses the hypothesis test described in https://arxiv.org/abs/1610.03944\n",
    "        for identifying the top category of a multinomial distribution.\n",
    "\n",
    "        :param x: the input [channel x height x width]\n",
    "        :param n: the number of Monte Carlo samples to use\n",
    "        :param alpha: the failure probability\n",
    "        :param batch_size: batch size to use when evaluating the base classifier\n",
    "        :return: the predicted class, or ABSTAIN\n",
    "        \"\"\"\n",
    "        self.base_classifier.eval()\n",
    "        counts = self._sample_noise(x, n, batch_size)\n",
    "        top2 = counts.argsort()[::-1][:2]\n",
    "        count1 = counts[top2[0]]\n",
    "        count2 = counts[top2[1]]\n",
    "        if binom_test(count1, count1 + count2, p=0.5) > alpha:\n",
    "            return Smooth.ABSTAIN\n",
    "        else:\n",
    "            return top2[0]\n",
    "\n",
    "    def _sample_noise(self, x: torch.tensor, num: int, batch_size) -> np.ndarray:\n",
    "        \"\"\" Sample the base classifier's prediction under noisy corruptions of the input x.\n",
    "\n",
    "        :param x: the input [channel x width x height]\n",
    "        :param num: number of samples to collect\n",
    "        :param batch_size:\n",
    "        :return: an ndarray[int] of length num_classes containing the per-class counts\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            counts = np.zeros(self.num_classes, dtype=int)\n",
    "            for _ in range(ceil(num / batch_size)):\n",
    "                this_batch_size = min(batch_size, num)\n",
    "                num -= this_batch_size\n",
    "\n",
    "                batch = x.repeat((this_batch_size, 1, 1, 1))\n",
    "                noise = torch.randn_like(batch, device='cuda') * self.sigma\n",
    "                predictions = self.base_classifier(batch + noise).argmax(1)\n",
    "                counts += self._count_arr(predictions.cpu().numpy(), self.num_classes)\n",
    "            return counts\n",
    "\n",
    "    def _count_arr(self, arr: np.ndarray, length: int) -> np.ndarray:\n",
    "        counts = np.zeros(length, dtype=int)\n",
    "        for idx in arr:\n",
    "            counts[idx] += 1\n",
    "        return counts\n",
    "\n",
    "    def _lower_confidence_bound(self, NA: int, N: int, alpha: float) -> float:\n",
    "        \"\"\" Returns a (1 - alpha) lower confidence bound on a bernoulli proportion.\n",
    "\n",
    "        This function uses the Clopper-Pearson method.\n",
    "\n",
    "        :param NA: the number of \"successes\"\n",
    "        :param N: the number of total draws\n",
    "        :param alpha: the confidence level\n",
    "        :return: a lower bound on the binomial proportion which holds true w.p at least (1 - alpha) over the samples\n",
    "        \"\"\"\n",
    "        return proportion_confint(NA, N, alpha=2 * alpha, method=\"beta\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "def getDataset(split: str) -> Dataset:\n",
    "    if split == \"train\":\n",
    "        return datasets.CIFAR10(\"./dataset_cache\", train=True, download=True, transform=transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor()\n",
    "        ]))\n",
    "    elif split == \"test\":\n",
    "        return datasets.CIFAR10(\"./dataset_cache\", train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "\n",
    "class NormalizeLayer(torch.nn.Module):\n",
    "    \"\"\"Standardize the channels of a batch of images by subtracting the dataset mean\n",
    "      and dividing by the dataset standard deviation.\n",
    "\n",
    "      In order to certify radii in original coordinates rather than standardized coordinates, we\n",
    "      add the Gaussian noise _before_ standardizing, which is why we have standardization be the first\n",
    "      layer of the classifier rather than as a part of preprocessing as is typical.\n",
    "      \"\"\"\n",
    "\n",
    "    def __init__(self, means: List[float], sds: List[float]):\n",
    "        \"\"\"\n",
    "        :param means: the channel means\n",
    "        :param sds: the channel standard deviations\n",
    "        \"\"\"\n",
    "        super(NormalizeLayer, self).__init__()\n",
    "        self.means = torch.tensor(means).cuda()\n",
    "        self.sds = torch.tensor(sds).cuda()\n",
    "\n",
    "    def forward(self, input: torch.tensor):\n",
    "        (batch_size, num_channels, height, width) = input.shape\n",
    "        means = self.means.repeat((batch_size, height, width, 1)).permute(0, 3, 1, 2)\n",
    "        sds = self.sds.repeat((batch_size, height, width, 1)).permute(0, 3, 1, 2)\n",
    "        return (input - means) / sds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.stats import norm, binom_test\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "\n",
    "class Smooth(object):\n",
    "    \"\"\"A smoothed classifier g \"\"\"\n",
    "\n",
    "    # to abstain, Smooth returns this int\n",
    "    ABSTAIN = -1\n",
    "\n",
    "    def __init__(self, base_classifier: torch.nn.Module, num_classes: int, sigma: float):\n",
    "        \"\"\"\n",
    "        :param base_classifier: maps from [batch x channel x height x width] to [batch x num_classes]\n",
    "        :param num_classes:\n",
    "        :param sigma: the noise level hyperparameter\n",
    "        \"\"\"\n",
    "        self.base_classifier = base_classifier\n",
    "        self.num_classes = num_classes\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def certify(self, x: torch.tensor, n0: int, n: int, alpha: float, batch_size: int) -> (int, float):\n",
    "        \"\"\" Monte Carlo algorithm for certifying that g's prediction around x is constant within some L2 radius.\n",
    "        With probability at least 1 - alpha, the class returned by this method will equal g(x), and g's prediction will\n",
    "        robust within a L2 ball of radius R around x.\n",
    "\n",
    "        :param x: the input [channel x height x width]\n",
    "        :param n0: the number of Monte Carlo samples to use for selection\n",
    "        :param n: the number of Monte Carlo samples to use for estimation\n",
    "        :param alpha: the failure probability\n",
    "        :param batch_size: batch size to use when evaluating the base classifier\n",
    "        :return: (predicted class, certified radius)\n",
    "                 in the case of abstention, the class will be ABSTAIN and the radius 0.\n",
    "        \"\"\"\n",
    "        self.base_classifier.eval()\n",
    "        # draw samples of f(x+ epsilon)\n",
    "        counts_selection = self._sample_noise(x, n0, batch_size)\n",
    "        # use these samples to take a guess at the top class\n",
    "        cAHat = counts_selection.argmax().item()\n",
    "        # draw more samples of f(x + epsilon)\n",
    "        counts_estimation = self._sample_noise(x, n, batch_size)\n",
    "        # use these samples to estimate a lower bound on pA\n",
    "        nA = counts_estimation[cAHat].item()\n",
    "        pABar = self._lower_confidence_bound(nA, n, alpha)\n",
    "        if pABar < 0.5:\n",
    "            return Smooth.ABSTAIN, 0.0\n",
    "        else:\n",
    "            radius = self.sigma * norm.ppf(pABar)\n",
    "            return cAHat, radius\n",
    "\n",
    "    def predict(self, x: torch.tensor, n: int, alpha: float, batch_size: int) -> int:\n",
    "        \"\"\" Monte Carlo algorithm for evaluating the prediction of g at x.  With probability at least 1 - alpha, the\n",
    "        class returned by this method will equal g(x).\n",
    "\n",
    "        This function uses the hypothesis test described in https://arxiv.org/abs/1610.03944\n",
    "        for identifying the top category of a multinomial distribution.\n",
    "\n",
    "        :param x: the input [channel x height x width]\n",
    "        :param n: the number of Monte Carlo samples to use\n",
    "        :param alpha: the failure probability\n",
    "        :param batch_size: batch size to use when evaluating the base classifier\n",
    "        :return: the predicted class, or ABSTAIN\n",
    "        \"\"\"\n",
    "        self.base_classifier.eval()\n",
    "        counts = self._sample_noise(x, n, batch_size)\n",
    "        top2 = counts.argsort()[::-1][:2]\n",
    "        count1 = counts[top2[0]]\n",
    "        count2 = counts[top2[1]]\n",
    "        if binom_test(count1, count1 + count2, p=0.5) > alpha:\n",
    "            return Smooth.ABSTAIN\n",
    "        else:\n",
    "            return top2[0]\n",
    "\n",
    "    def _sample_noise(self, x: torch.tensor, num: int, batch_size) -> np.ndarray:\n",
    "        \"\"\" Sample the base classifier's prediction under noisy corruptions of the input x.\n",
    "\n",
    "        :param x: the input [channel x width x height]\n",
    "        :param num: number of samples to collect\n",
    "        :param batch_size:\n",
    "        :return: an ndarray[int] of length num_classes containing the per-class counts\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            counts = np.zeros(self.num_classes, dtype=int)\n",
    "            for _ in range(ceil(num / batch_size)):\n",
    "                this_batch_size = min(batch_size, num)\n",
    "                num -= this_batch_size\n",
    "\n",
    "                batch = x.repeat((this_batch_size, 1, 1, 1))\n",
    "                noise = torch.randn_like(batch, device='cuda') * self.sigma\n",
    "                predictions = self.base_classifier(batch + noise).argmax(1)\n",
    "                counts += self._count_arr(predictions.cpu().numpy(), self.num_classes)\n",
    "            return counts\n",
    "\n",
    "    def _count_arr(self, arr: np.ndarray, length: int) -> np.ndarray:\n",
    "        counts = np.zeros(length, dtype=int)\n",
    "        for idx in arr:\n",
    "            counts[idx] += 1\n",
    "        return counts\n",
    "\n",
    "    def _lower_confidence_bound(self, NA: int, N: int, alpha: float) -> float:\n",
    "        \"\"\" Returns a (1 - alpha) lower confidence bound on a bernoulli proportion.\n",
    "\n",
    "        This function uses the Clopper-Pearson method.\n",
    "\n",
    "        :param NA: the number of \"successes\"\n",
    "        :param N: the number of total draws\n",
    "        :param alpha: the confidence level\n",
    "        :return: a lower bound on the binomial proportion which holds true w.p at least (1 - alpha) over the samples\n",
    "        \"\"\"\n",
    "        return proportion_confint(NA, N, alpha=2 * alpha, method=\"beta\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar_resnet110\n"
     ]
    }
   ],
   "source": [
    "_CIFAR10_MEAN = [0.4914, 0.4822, 0.4465]\n",
    "_CIFAR10_STDDEV = [0.2023, 0.1994, 0.2010]\n",
    "\n",
    "\n",
    "# load the base classifier\n",
    "checkpoint = torch.load(\"cifar10Gaussian/resnet110/noise_0.25/checkpoint.pth.tar\")\n",
    "model = resnet_cifar(depth=110, num_classes=10).cuda()\n",
    "normalize_layer = NormalizeLayer(_CIFAR10_MEAN, _CIFAR10_STDDEV)\n",
    "base_classifier = torch.nn.Sequential(normalize_layer, model)\n",
    "base_classifier.load_state_dict(checkpoint['state_dict'])\n",
    "sigma=0.25\n",
    "smoothed_classifier = Smooth(base_classifier, 10, 0.25)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "0\t3\t3\t0.377\t1\n",
      "1\t8\t8\t0.466\t1\n",
      "2\t8\t8\t0.58\t1\n",
      "3\t0\t0\t0.601\t1\n",
      "4\t6\t6\t0.359\t1\n",
      "5\t6\t6\t0.546\t1\n",
      "6\t1\t1\t0.385\t1\n",
      "7\t6\t-1\t0.0\t0\n",
      "8\t3\t3\t0.25\t1\n",
      "9\t1\t1\t0.569\t1\n",
      "10\t0\t0\t0.0866\t1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# iterate through the dataset\n",
    "dataset = getDataset(\"test\")\n",
    "maxItr = 500\n",
    "skip = 1\n",
    "\n",
    "N0 = 100\n",
    "N = 100000\n",
    "alpha = 0.001\n",
    "batch = 1000\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    # and stop after args.max examples\n",
    "    if i % skip != 0:\n",
    "        continue\n",
    "    if i == maxItr:\n",
    "        break\n",
    "\n",
    "    (x, label) = dataset[i]\n",
    "\n",
    "    #before_time = time()\n",
    "    # certify the prediction of g around x\n",
    "    x = x.cuda()\n",
    "    prediction, radius = smoothed_classifier.certify(x, N0, N, alpha, batch)\n",
    "    #after_time = time()\n",
    "    correct = int(prediction == label)\n",
    "\n",
    "    #time_elapsed = str(datetime.timedelta(seconds=(after_time - before_time)))\n",
    "    print(\"{}\\t{}\\t{}\\t{:.3}\\t{}\".format(\n",
    "            i, label, prediction, radius, correct), flush=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchSurvey",
   "language": "python",
   "name": "torchsurvey"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
