{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "#import torch.backends.cudnn as cudnn\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#PGD code adapted from https://github.com/Harry24k/adversarial-attacks-pytorch\n",
    "\n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, mean, std) :\n",
    "        super(Normalize, self).__init__()\n",
    "        self.register_buffer('mean', torch.Tensor(mean))\n",
    "        self.register_buffer('std', torch.Tensor(std))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # Broadcasting\n",
    "        mean = self.mean.reshape(1, 3, 1, 1)\n",
    "        std = self.std.reshape(1, 3, 1, 1)\n",
    "        return (input - mean) / std\n",
    "\n",
    "\n",
    "norm_layer = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "\n",
    "model = nn.Sequential(\n",
    "    norm_layer,\n",
    "    resnet50(weights=weights))\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "if device == 'cuda':\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "model = model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "\n",
    "def show(imgs):\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = T.ToPILImage()(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.Normalize(mean=[0, 0, 0], std=[255.0, 255.0, 255.0]),\n",
    "    \n",
    "])\n",
    "\n",
    "        \n",
    "img = read_image('images/black_swan.jpeg')\n",
    "#img = read_image('images/macaw.jpg')\n",
    "show([img])\n",
    "batch = transform(img.float()).unsqueeze(0)\n",
    "print(img.shape,batch.shape)\n",
    "show(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batch.to(device)\n",
    "prediction = model(batch).squeeze(0).softmax(0)\n",
    "\n",
    "class_id = prediction.argmax().item()\n",
    "score = prediction[class_id].item()\n",
    "category_name = weights.meta[\"categories\"][class_id]\n",
    "print(\"Original Label: \"f\"{category_name} {100 * score:.1f}%\")\n",
    "print(\"Top-K labels:\",prediction.topk(5)[1].cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PGD(model, x, y, loss_fn, num_steps, step_size, step_norm, eps, eps_norm,\n",
    "                               clamp=(0,1), y_target=None):\n",
    "\n",
    "    x_adv = x.clone().detach().requires_grad_(True).to(x.device)\n",
    "    targeted = y_target is not None\n",
    "    num_channels = x.shape[1]\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        _x_adv = x_adv.clone().detach().requires_grad_(True)\n",
    "\n",
    "        prediction = model(_x_adv)\n",
    "        loss = loss_fn(prediction, y_target if targeted else y)\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Force the gradient step to be a fixed size in a certain norm\n",
    "            if step_norm == 'inf':\n",
    "                gradients = _x_adv.grad.sign() * step_size\n",
    "            else:\n",
    "                # Note .view() assumes batched image data as 4D tensor\n",
    "                gradients = _x_adv.grad * step_size / _x_adv.grad.view(_x_adv.shape[0], -1)\\\n",
    "                    .norm(step_norm, dim=-1)\\\n",
    "                    .view(-1, 1, 1, 1)\n",
    "\n",
    "            if targeted:\n",
    "                # Targeted: Gradient descent with on the loss of the (incorrect) target label\n",
    "                # w.r.t. the image data\n",
    "                x_adv -= gradients\n",
    "            else:\n",
    "                # Untargeted: Gradient ascent on the loss of the correct label w.r.t.\n",
    "                # the model parameters\n",
    "                x_adv += gradients\n",
    "\n",
    "        # Project back into l_norm ball and correct range\n",
    "        if eps_norm == 'inf':\n",
    "            # Workaround as PyTorch doesn't have elementwise clip\n",
    "            x_adv = torch.max(torch.min(x_adv, x + eps), x - eps)\n",
    "        else:\n",
    "            delta = x_adv - x\n",
    "\n",
    "            # Assume x and x_adv are batched tensors where the first dimension is\n",
    "            # a batch dimension\n",
    "            mask = delta.view(delta.shape[0], -1).norm(eps_norm, dim=1) <= eps\n",
    "\n",
    "            scaling_factor = delta.view(delta.shape[0], -1).norm(eps_norm, dim=1)\n",
    "            scaling_factor[mask] = eps\n",
    "\n",
    "            # .view() assumes batched images as a 4D Tensor\n",
    "            delta *= eps / scaling_factor.view(-1, 1, 1, 1)\n",
    "\n",
    "            x_adv = x + delta\n",
    "            \n",
    "        x_adv = x_adv.clamp(*clamp)\n",
    "    \n",
    "    delta = x_adv-x\n",
    "    return x_adv.detach(), delta \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 100\n",
    "eps = 8/255.0\n",
    "step_size = 2.5*eps*0.01 \n",
    "step_norm = 'inf'\n",
    "eps_norm = 'inf'\n",
    "#step_norm = 2\n",
    "#eps_norm = 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "target = nn.functional.one_hot(torch.tensor(987), num_classes=1000).unsqueeze(0).float().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "perturbed_data, delta = PGD(model, batch, torch.tensor([class_id]).to(device), criterion, num_steps, step_size, step_norm, eps, eps_norm,clamp=(0,1),y_target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(perturbed_data).squeeze(0).softmax(0)\n",
    "class_id = prediction.argmax().item()\n",
    "score = prediction[class_id].item()\n",
    "category_name = weights.meta[\"categories\"][class_id]\n",
    "print(\"Attacked Label: \"f\"{category_name} {100 * score:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(perturbed_data)\n",
    "a = delta.min().cpu().detach()\n",
    "b = delta.max().cpu().detach()\n",
    "scaledDelta = (delta-a)/(b-a)\n",
    "show(scaledDelta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchSurvey",
   "language": "python",
   "name": "torchsurvey"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
